{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import  mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed_trips_2018.csv')\n",
    "stations = pd.read_csv('../data/processed_stations_2018.csv')\n",
    "weather = pd.read_csv('../data/weather_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec766923",
   "metadata": {},
   "source": [
    "We assign the station clusters to the start station ids because this is where the bikes are going to be picked up from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"station_cluster\"] = data[\"start_station_id\"].map(stations.set_index(\"station_id\")[\"station_cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cols = [\"datetime\",\"temp\",\"feelslike\",\"humidity\",\"precip\",\"precipprob\",\"snow\",\"snowdepth\",\"windspeed\",\"sealevelpressure\",\"visibility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e162a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cleaned = weather[weather_cols]\n",
    "weather_cleaned['datetime'] = pd.to_datetime(weather_cleaned['datetime'])\n",
    "\n",
    "#backfill missing weather data\n",
    "weather_cleaned = weather_cleaned.sort_values('datetime').set_index('datetime').asfreq('h')\n",
    "weather_cleaned = weather_cleaned.fillna(method='ffill')\n",
    "weather_cleaned = weather_cleaned.reset_index()\n",
    "\n",
    "weather_cleaned[\"day_of_week\"] = weather_cleaned[\"datetime\"].dt.dayofweek\n",
    "weather_cleaned[\"hour_of_day\"] = weather_cleaned[\"datetime\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af000226",
   "metadata": {},
   "source": [
    "### Check how many bikes on average per hour per cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b2183",
   "metadata": {},
   "source": [
    "To select the best model to use, we first identify the busiest clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112441a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_per_day = data.groupby(['station_cluster', 'pickup_datetime']).size().reset_index(name='num_rentals')\n",
    "avg_bikes_per_cluster = bikes_per_day.groupby('station_cluster')['num_rentals'].mean().sort_values(ascending=False).reset_index(name='avg_num_rentals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = data[data['station_cluster'] == avg_bikes_per_cluster.iloc[0]['station_cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ec0bc",
   "metadata": {},
   "source": [
    "## Creation of the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a64a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_pickups_cluster = cluster.groupby(['pickup_datetime'])['bikeid'].size().reset_index(name='num_rentals')\n",
    "hourly_dropoffs_cluster = cluster.groupby(['dropoff_datetime'])['bikeid'].size().reset_index(name='num_rentals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_cluster = pd.merge(hourly_pickups_cluster, hourly_dropoffs_cluster, left_on='pickup_datetime', right_on='dropoff_datetime', how='outer', suffixes=('_pickups', '_dropoffs')).drop(columns=['dropoff_datetime']).rename(columns={'pickup_datetime': 'datetime'})\n",
    "hourly_cluster['datetime'] = pd.to_datetime(hourly_cluster['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_cluster = hourly_cluster[hourly_cluster['datetime'] < '2019-01-01']\n",
    "hourly_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f279dcd",
   "metadata": {},
   "source": [
    "### We ensure each timestamp gets a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of hourly_cluster: {hourly_cluster.shape}')\n",
    "def fill_missing_hours(df):\n",
    "    all_hours = pd.date_range(start=df['datetime'].min(), end=df['datetime'].max(), freq='h')\n",
    "    all_hours_df = pd.DataFrame({'datetime': all_hours})\n",
    "    all_hours_df[\"datetime\"] = pd.to_datetime(all_hours_df[\"datetime\"])\n",
    "    merged_df = pd.merge(all_hours_df, df, on=['datetime'], how='left')\n",
    "    merged_df['num_rentals_pickups'] = merged_df['num_rentals_pickups'].fillna(0).astype(int)\n",
    "    merged_df['num_rentals_dropoffs'] = merged_df['num_rentals_dropoffs'].fillna(0).astype(int)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "hourly_cluster = fill_missing_hours(hourly_cluster)\n",
    "print(f'size of hourly_cluster after filling missing hours: {hourly_cluster.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408829a8",
   "metadata": {},
   "source": [
    "## Initial Exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def plot_acf_pacf(series, lags=50, title_suffix=''):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "    \n",
    "    plot_acf(series, lags=lags, ax=axes[0])\n",
    "    axes[0].set_title(f'ACF {title_suffix}')\n",
    "    \n",
    "    plot_pacf(series, lags=lags, ax=axes[1], method='ywm') \n",
    "    axes[1].set_title(f'PACF {title_suffix}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_acf_pacf(hourly_cluster['num_rentals_pickups'], lags=50, title_suffix='for Bike Rentals Pickups')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3df86",
   "metadata": {},
   "source": [
    "ACF: decays slowly, crosses significance after lag 5, then rises at multiples of 24 hours.\n",
    "\n",
    "PACF: significant at lag 0 and 1 positive, lag 2 negative, then essentially small values.\n",
    "\n",
    "Slow decay in ACF → indicates strong autocorrelation / possible AR component.\n",
    "\n",
    "PACF spikes at lag 1 → suggests AR(1) term is relevant.\n",
    "\n",
    "ACF peaks at multiples of 24 hours → indicates daily seasonality (hourly data).\n",
    "\n",
    "Beyond lag 5, ACF is mostly noise → MA terms may be limited to small lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26070ffc",
   "metadata": {},
   "source": [
    "## Train Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CONSTANTS\n",
    "END_TRAINING_DATE = pd.to_datetime(\"2018-09-01 00:00:00\")\n",
    "END_VALIDATION_DATE = pd.to_datetime(\"2018-11-01 00:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98aa10",
   "metadata": {},
   "source": [
    "## Split train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = hourly_cluster[hourly_cluster['datetime'] < END_VALIDATION_DATE].set_index('datetime')\n",
    "test_data = hourly_cluster[hourly_cluster['datetime'] >= END_VALIDATION_DATE].set_index('datetime')\n",
    "\n",
    "train_data.index = pd.to_datetime(train_data.index)\n",
    "test_data.index  = pd.to_datetime(test_data.index)\n",
    "\n",
    "train_weather  = weather_cleaned[weather_cleaned['datetime'] < END_VALIDATION_DATE].set_index('datetime')\n",
    "test_weather   = weather_cleaned[weather_cleaned['datetime'] >= END_VALIDATION_DATE].set_index('datetime')\n",
    "\n",
    "train_weather.index = pd.to_datetime(train_weather.index)\n",
    "test_weather.index  = pd.to_datetime(test_weather.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.asfreq('h')\n",
    "test_data = test_data.asfreq('h')\n",
    "\n",
    "train_weather = train_weather.asfreq('h')\n",
    "test_weather = test_weather.asfreq('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c35a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exog = train_weather[['temp','feelslike','humidity','precip','precipprob','snowdepth','windspeed','visibility']]\n",
    "test_exog = test_weather[['temp','feelslike','humidity','precip','precipprob','snowdepth','windspeed','visibility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99563519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mse), 2)\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    return mae, rmse\n",
    "\n",
    "def plot_predictions_vs_true(predicted_values, true_values):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(true_values, label='Actual', linewidth=2)\n",
    "    plt.plot(predicted_values, label='Predicted', linewidth=2)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Number of Rentals')\n",
    "    plt.title('Actual vs Predicted Number of Rentals')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKUP_TARGET_COL = 'num_rentals_pickups'\n",
    "DROPOFF_TARGET_COL = 'num_rentals_dropoffs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_pickup = {}\n",
    "performance_dropoffs= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab6d7c",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877b8c8",
   "metadata": {},
   "source": [
    "This fold_config is used to run a cross validation time series with two folds. The best model selected will be the one with the lowers average MAE between folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b279a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_config = [\n",
    "    {'train_start': pd.Timestamp('2018-01-01 00:00:00'), \n",
    "     'train_end': pd.Timestamp('2018-06-30 23:00:00'),\n",
    "     'val_start': pd.Timestamp('2018-07-01 00:00:00'), \n",
    "     'val_end': pd.Timestamp('2018-08-31 23:00:00')},\n",
    "    {'train_start': pd.Timestamp('2018-01-01 00:00:00'), \n",
    "     'train_end': pd.Timestamp('2018-08-31 23:00:00'),\n",
    "     'val_start': pd.Timestamp('2018-09-01 00:00:00'), \n",
    "     'val_end': pd.Timestamp('2018-10-31 23:00:00')}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f217de",
   "metadata": {},
   "source": [
    "## Predictions Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def sarimax_fit_predict_folds_daily(folds_config, train_data, target, order, s_order, exog=None):\n",
    "    \"\"\"\n",
    "    Fits a sarima or sarimax model (if exog) for each fold in the cross validation\n",
    "    and predicts only at midnight for the next 24 hours\n",
    "    \"\"\"    \n",
    "    maes = []\n",
    "    rmses= []\n",
    "    for i, fold in enumerate(folds_config, 1):\n",
    "        trues= []\n",
    "        preds = []\n",
    "        train_fold = train_data.loc[fold['train_start']:fold['train_end']]\n",
    "        val_fold   = train_data.loc[fold['val_start']:fold['val_end']].copy()\n",
    "\n",
    "        val_fold_midnight = val_fold[val_fold.index.hour == 0]\n",
    "\n",
    "        if exog is not None:\n",
    "            exog_train = exog.loc[fold['train_start']:fold['train_end']]\n",
    "            exog_val   = exog.loc[val_fold_midnight.index[0]:val_fold_midnight.index[-1]+pd.Timedelta(hours=23)]\n",
    "        else:\n",
    "            exog_train = None\n",
    "            exog_val = None\n",
    "\n",
    "        # Fit SARIMAX\n",
    "        model = SARIMAX(\n",
    "            train_fold[target],\n",
    "            exog=exog_train,\n",
    "            order=order,\n",
    "            seasonal_order=s_order,\n",
    "            enforce_stationarity=True,\n",
    "            enforce_invertibility=True\n",
    "        )\n",
    "        model_fit = model.fit(disp=False)\n",
    "\n",
    "        for start_time in val_fold_midnight.index:\n",
    "            end_time = start_time + pd.Timedelta(hours=23)\n",
    "            if exog_val is not None:\n",
    "                exog_slice = exog_val.loc[start_time:end_time]\n",
    "            else:\n",
    "                exog_slice = None\n",
    "\n",
    "            pred = model_fit.forecast(steps=24, exog=exog_slice)\n",
    "            pred = np.clip(pred.values, 0, None)\n",
    "\n",
    "            true = val_fold.loc[start_time:end_time, target]\n",
    "\n",
    "            trues.append(true)\n",
    "            preds.append(pd.Series(pred, index=true.index))\n",
    "        \n",
    "        trues = pd.concat(trues)\n",
    "        preds = pd.concat(preds)\n",
    "        if i == 2:\n",
    "            print(f\"Fold {i} SARIMAX summary:\")\n",
    "            print(model_fit.summary())\n",
    "            plot_predictions_vs_true(preds, trues)\n",
    "\n",
    "        mae_fold, rmse_fold = print_metrics(trues, preds)\n",
    "        maes.append(mae_fold)\n",
    "        rmses.append(rmse_fold)\n",
    "\n",
    "    avg_mae = np.round(np.mean(maes),2)\n",
    "    avg_rmse = np.round(np.mean(rmses),2)\n",
    "    return avg_mae, avg_rmse\n",
    "\n",
    "\n",
    "def sarimax_fit_predict(train_data, test_data, target, order, s_order, new_col, exog_train=None, exog_test=None):\n",
    "    \"\"\" Fits a sarima or sarimax model (if exog) and predicts only at midnight for the next 24 hours \"\"\" \n",
    "    model = SARIMAX(\n",
    "        train_data[target],\n",
    "        exog=exog_train,\n",
    "        order=order,             \n",
    "        seasonal_order=s_order, \n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    model_fit = model.fit(disp=False)\n",
    "    print(model_fit.summary())\n",
    "\n",
    "    trues = []\n",
    "    preds = []\n",
    "\n",
    "    test_midnight = test_data[test_data.index.hour == 0]\n",
    "\n",
    "    for start_time in test_midnight.index:\n",
    "        end_time = start_time + pd.Timedelta(hours=23)\n",
    "        if exog_test is not None:\n",
    "            exog_slice = exog_test.loc[start_time:end_time]\n",
    "        else:\n",
    "            exog_slice = None\n",
    "\n",
    "        # Forecast next 24 hours\n",
    "        pred = model_fit.forecast(steps=24, exog=exog_slice)\n",
    "        pred = np.clip(pred.values, 0, None)\n",
    "\n",
    "        # True values\n",
    "        true = test_data.loc[start_time:end_time, target]\n",
    "\n",
    "        trues.append(true)\n",
    "        preds.append(pd.Series(pred, index=true.index))\n",
    "\n",
    "    # Concatenate all days\n",
    "    trues = pd.concat(trues)\n",
    "    preds = pd.concat(preds)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae, rmse = print_metrics(trues, preds)\n",
    "\n",
    "    # Plot predictions vs true values\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Actual': trues,\n",
    "        'Predicted': preds\n",
    "    })\n",
    "    plot_predictions_vs_true(df_plot[\"Predicted\"], df_plot['Actual'])\n",
    "\n",
    "    return mae, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2eaa4",
   "metadata": {},
   "source": [
    "## 1. First Approach: given the ACF and PACF we try a SARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "order=(1,1,1)\n",
    "s_order=(0,1,1,24)\n",
    "\n",
    "print(f'Prediciting {PICKUP_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, PICKUP_TARGET_COL, order,s_order)\n",
    "performance_pickup[\"model_sarima\"] = {\"val_mae\": mae, \"val_rmse\": rmse}\n",
    "\n",
    "print(f'Prediciting {DROPOFF_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, DROPOFF_TARGET_COL, order,s_order)\n",
    "performance_dropoffs[\"model_sarima\"] = {\"val_mae\": mae, \"val_rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858f0d7",
   "metadata": {},
   "source": [
    "## 2. We add exogenous weather variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediciting {PICKUP_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, PICKUP_TARGET_COL, order,s_order, train_exog)\n",
    "performance_pickup[\"model_sarimax\"] = {\"val_mae\": mae, \"val_rmse\": rmse}\n",
    "\n",
    "print(f'Prediciting {DROPOFF_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, DROPOFF_TARGET_COL, order,s_order, train_exog)\n",
    "performance_dropoffs[\"model_sarimax\"] = {\"val_mae\": mae, \"val_rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbf9a5",
   "metadata": {},
   "source": [
    "## 3. We keep only the relevant exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f632b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exog_short = train_exog[['temp','precip','visibility']]\n",
    "test_exog_short = test_exog[['temp','precip','visibility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a176bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediciting {PICKUP_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, PICKUP_TARGET_COL, order,s_order, train_exog_short)\n",
    "performance_pickup[\"model_sarimax_short\"] = {\"val_mae\": mae, \"val_rmse\": rmse}\n",
    "\n",
    "print(f'Prediciting {DROPOFF_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, DROPOFF_TARGET_COL, order,s_order, train_exog_short)\n",
    "performance_dropoffs[\"model_sarimax_short\"] = {\"val_mae\": mae, \"val_rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820322a2",
   "metadata": {},
   "source": [
    "## 4. Adding day and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31860dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exog_w_date = train_weather[['temp','precip','visibility', 'day_of_week', 'hour_of_day']]\n",
    "\n",
    "train_exog_w_date['hour_sin'] = np.sin(2 * np.pi * train_exog_w_date['hour_of_day'] / 24)\n",
    "train_exog_w_date['hour_cos'] = np.cos(2 * np.pi * train_exog_w_date['hour_of_day'] / 24)\n",
    "train_exog_w_date = train_exog_w_date.drop('hour_of_day', axis=1)\n",
    "\n",
    "train_exog_w_date['dow_sin'] = np.sin(2 * np.pi * train_exog_w_date['day_of_week'] / 7)\n",
    "train_exog_w_date['dow_cos'] = np.cos(2 * np.pi * train_exog_w_date['day_of_week'] / 7)\n",
    "train_exog_w_date = train_exog_w_date.drop('day_of_week', axis=1)\n",
    "\n",
    "test_exog_w_date = test_weather[['temp','precip','visibility', 'day_of_week', 'hour_of_day']]\n",
    "\n",
    "test_exog_w_date['hour_sin'] = np.sin(2 * np.pi * test_exog_w_date['hour_of_day'] / 24)\n",
    "test_exog_w_date['hour_cos'] = np.cos(2 * np.pi * test_exog_w_date['hour_of_day'] / 24)\n",
    "test_exog_w_date = test_exog_w_date.drop('hour_of_day', axis=1)\n",
    "\n",
    "test_exog_w_date['dow_sin'] = np.sin(2 * np.pi * test_exog_w_date['day_of_week'] / 7)\n",
    "test_exog_w_date['dow_cos'] = np.cos(2 * np.pi * test_exog_w_date['day_of_week'] / 7)\n",
    "test_exog_w_date = test_exog_w_date.drop('day_of_week', axis=1)\n",
    "\n",
    "print(f'Prediciting {PICKUP_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, PICKUP_TARGET_COL, order,s_order, train_exog_w_date)\n",
    "performance_pickup[\"model_sarimax_short_w_date\"] = {\"val_mae\": mae, \"val_rmse\": rmse}\n",
    "\n",
    "print(f'Prediciting {DROPOFF_TARGET_COL}')\n",
    "mae, rmse = sarimax_fit_predict_folds_daily(folds_config, train_data, DROPOFF_TARGET_COL, order,s_order, train_exog_w_date)\n",
    "performance_dropoffs[\"model_sarimax_short_w_date\"] = {\"val_mae\": mae, \"val_rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd4650",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e11b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b51a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(hourly_cluster,n_lags, forecast_horizon, target):\n",
    "    train_data_nn = hourly_cluster.set_index('datetime')\n",
    "    # --- Create lag features ---\n",
    "    df = train_data_nn[[target]].copy()\n",
    "    lags= {f'target_lag_{lag}': df[target].shift(lag) for lag in range(1, n_lags+1)}\n",
    "    lags_df = pd.DataFrame(lags, index=df.index)\n",
    "    df = pd.concat([df, lags_df], axis=1)\n",
    "\n",
    "    exog_features = ['day_of_week','hour_of_day', 'precip', 'visibility']\n",
    "    df[exog_features] = weather_cleaned.set_index('datetime')[exog_features].loc[df.index]\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "    df = df.drop(['day_of_week','hour_of_day'], axis=1)\n",
    "    exog_features = ['precip', 'visibility','hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "    # --- Create multi-step target ---\n",
    "    for step in range(1, forecast_horizon + 1):\n",
    "        df[f'target_+{step}'] = df[target].shift(-step)\n",
    "\n",
    "    # --- Drop rows with NaNs due to lags or forward targets ---\n",
    "    all_columns = [f'target_lag_{lag}' for lag in range(1, n_lags + 1)] + exog_features + \\\n",
    "              [f'target_+{step}' for step in range(1, forecast_horizon + 1)]\n",
    "\n",
    "    df = df.dropna(subset=all_columns)\n",
    "    # --- Keep only rows at midnight (00:00) for end-of-day prediction ---\n",
    "    df = df[df.index.hour == 0]\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Parameters ---\n",
    "n_lags = 168        # previous week of hourly data\n",
    "forecast_horizon = 24  # next 24 hours\n",
    "\n",
    "df_pickup = get_dataset(hourly_cluster, n_lags, forecast_horizon, PICKUP_TARGET_COL)\n",
    "df_dropoff =get_dataset(hourly_cluster, n_lags, forecast_horizon, DROPOFF_TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datesets(df, END_TRAINING_DATE, END_VALIDATION_DATE, exog_features, forecast_horizon):\n",
    "\n",
    "    X = df[[f'target_lag_{lag}' for lag in range(1, n_lags+1)] + exog_features].values\n",
    "    y = df[[f'target_+{step}' for step in range(1, forecast_horizon+1)]].values\n",
    "\n",
    "    X_train = X[df.index < END_TRAINING_DATE]\n",
    "    y_train = y[df.index < END_TRAINING_DATE]\n",
    "\n",
    "    X_val = X[(df.index >= END_TRAINING_DATE) & (df.index < END_VALIDATION_DATE)]\n",
    "    y_val = y[(df.index >= END_TRAINING_DATE) & (df.index < END_VALIDATION_DATE)]\n",
    "\n",
    "    X_train_final = X[df.index < END_VALIDATION_DATE]\n",
    "    y_train_final = y[df.index < END_VALIDATION_DATE]\n",
    "\n",
    "    X_test = X[df.index >= END_VALIDATION_DATE]\n",
    "    y_test = y[df.index >= END_VALIDATION_DATE]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_train_final, y_train_final, X_test, y_test\n",
    "\n",
    "exog_features = ['precip', 'visibility','hour_sin', 'hour_cos', 'dow_sin', 'dow_cos']\n",
    "\n",
    "X_p_train, y_p_train, X_p_val, y_p_val, X_p_train_final, y_p_train_final, X_p_test, y_p_test = split_datesets(df_pickup, END_TRAINING_DATE, END_VALIDATION_DATE,exog_features,forecast_horizon)\n",
    "\n",
    "X_d_train, y_d_train, X_d_val, y_d_val, X_d_train_final, y_d_train_final, X_d_test, y_d_test = split_datesets(df_dropoff, END_TRAINING_DATE, END_VALIDATION_DATE,exog_features,forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e76f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pickups\n",
    "scaler_pickup = StandardScaler()\n",
    "X_pickup_train_scaled = scaler_pickup.fit_transform(X_p_train)\n",
    "X_pickup_val_scaled = scaler_pickup.transform(X_p_val)\n",
    "\n",
    "# Dropoffs\n",
    "scaler_dropoff = StandardScaler()\n",
    "X_dropoff_train_scaled = scaler_dropoff.fit_transform(X_d_train)\n",
    "X_dropoff_val_scaled = scaler_dropoff.transform(X_d_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_nn_and_train(X_train_scaled, y_train):\n",
    "\n",
    "    model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)), \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(24) \n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=24,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def predict_nn(model, X_val_scaled, y_val, val_start, val_end):\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "    # Flatten for all predictions\n",
    "    y_val_flat = y_val.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "\n",
    "    # Metrics\n",
    "    mae, rmse = print_metrics(y_val_flat, y_pred_flat)\n",
    "\n",
    "    hourly_index = pd.date_range(start=val_start, end=val_end, freq='h')\n",
    "\n",
    "    df_hourly_val = pd.DataFrame(index=hourly_index)\n",
    "    df_hourly_val[\"y_pred\"] = y_pred_flat\n",
    "    df_hourly_val[\"y_true\"] = y_val_flat\n",
    "    plot_predictions_vs_true(df_hourly_val[\"y_pred\"], df_hourly_val[\"y_true\"])\n",
    "    return y_val_flat, y_pred_flat, mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3595d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_start = pd.Timestamp('2018-09-01 00:00:00')\n",
    "val_end   = pd.Timestamp('2018-10-31 23:00:00')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_model = create_nn_and_train(X_pickup_train_scaled, y_p_train)\n",
    "y_val_flat, y_pred_flat, mae_p_train, rmse_p_train = predict_nn(p_train_model, X_pickup_val_scaled, y_p_val, val_start, val_end)\n",
    "\n",
    "performance_pickup[\"model_nn\"] = {\"val_mae\": np.round(np.mean(mae_p_train),2), \"val_rmse\": np.round(np.mean(rmse_p_train),2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb955f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_model = create_nn_and_train(X_dropoff_train_scaled, y_d_train)\n",
    "y_val_flat_d, y_pred_flat_d, mae_d_train, rmse_d_train = predict_nn(d_train_model, X_dropoff_val_scaled, y_d_val, val_start, val_end)\n",
    "performance_dropoffs[\"model_nn\"] = {\"val_mae\": np.round(np.mean(mae_d_train),2), \"val_rmse\": np.round(np.mean(rmse_d_train),2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d352cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_pickup, performance_dropoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0534fc8",
   "metadata": {},
   "source": [
    "## Performance of best model in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00f96f",
   "metadata": {},
   "source": [
    "### Pick ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pickup = StandardScaler()\n",
    "X_pickup_final_scaled = scaler_pickup.fit_transform(X_p_train_final)\n",
    "X_pickup_test_scaled = scaler_pickup.transform(X_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc83d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_start = pd.Timestamp('2018-11-01 00:00:00')\n",
    "val_end   = pd.Timestamp('2018-12-30 23:00:00')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229df6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final_model = create_nn_and_train(X_pickup_final_scaled, y_p_train_final)\n",
    "y_val_flat_final, y_pred_flat_final, mae_p_final, rmse_p_final = predict_nn(p_final_model, X_pickup_test_scaled, y_p_test, val_start, val_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ac75d",
   "metadata": {},
   "source": [
    "### Drop Offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dropoff = StandardScaler()\n",
    "X_dropoff_final_scaled = scaler_dropoff.fit_transform(X_d_train_final)\n",
    "X_dropoff_test_scaled = scaler_dropoff.transform(X_d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_final_model = create_nn_and_train(X_dropoff_final_scaled, y_d_train_final)\n",
    "y_val_flat_d_final, y_pred_flat_d_final, mae_d_final, rmse_d_final = predict_nn(d_final_model, X_dropoff_test_scaled, y_d_test, val_start, val_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bde74",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_bikes_per_cluster={}\n",
    "\n",
    "for cluster_id in avg_bikes_per_cluster[\"station_cluster\"]:\n",
    "    cluster = data[data['station_cluster'] == cluster_id]\n",
    "    print(f'cluster_id: {cluster_id}')\n",
    "    #create hourly cluster dataset\n",
    "    hourly_pickups_cluster = cluster.groupby(['pickup_datetime'])['bikeid'].size().reset_index(name='num_rentals')\n",
    "    hourly_dropoffs_cluster = cluster.groupby(['dropoff_datetime'])['bikeid'].size().reset_index(name='num_rentals')\n",
    "    hourly_cluster = pd.merge(hourly_pickups_cluster, hourly_dropoffs_cluster, left_on='pickup_datetime', right_on='dropoff_datetime', how='outer', suffixes=('_pickups', '_dropoffs')).drop(columns=['dropoff_datetime']).rename(columns={'pickup_datetime': 'datetime'})\n",
    "    hourly_cluster['datetime'] = pd.to_datetime(hourly_cluster['datetime'])\n",
    "    hourly_cluster = hourly_cluster[hourly_cluster['datetime'] < '2019-01-01']\n",
    "    \n",
    "    #fill empty timestamps\n",
    "    print(f'size of hourly_cluster: {hourly_cluster.shape}')\n",
    "    hourly_cluster = fill_missing_hours(hourly_cluster)\n",
    "    print(f'size of hourly_cluster after filling missing hours: {hourly_cluster.shape}')\n",
    "\n",
    "    train_data = hourly_cluster.set_index('datetime')\n",
    "    train_data.index = pd.to_datetime(train_data.index)\n",
    "\n",
    "    df_pickup = get_dataset(hourly_cluster, n_lags, forecast_horizon, PICKUP_TARGET_COL)\n",
    "    df_dropoff =get_dataset(hourly_cluster, n_lags, forecast_horizon, DROPOFF_TARGET_COL)\n",
    "\n",
    "    #use the last day for prediction  \n",
    "    last_pickup =  df_pickup.iloc[-1].copy()\n",
    "    df_pickup_train = df_pickup.iloc[:-1].copy()\n",
    "    last_dropoff =  df_dropoff.iloc[-1].copy()\n",
    "    df_dropoff_train = df_dropoff.iloc[:-1].copy()\n",
    "\n",
    "    #select only relevant features\n",
    "    X_pickup = df_pickup_train[[f'target_lag_{lag}' for lag in range(1, n_lags+1)] + exog_features].values\n",
    "    y_pickup = df_pickup_train[[f'target_+{step}' for step in range(1, forecast_horizon+1)]].values\n",
    "    last_pickup_to_predict = last_pickup[[f'target_lag_{lag}' for lag in range(1, n_lags+1)] + exog_features].values.reshape(1, -1)\n",
    "  \n",
    "    X_dropoff = df_dropoff_train[[f'target_lag_{lag}' for lag in range(1, n_lags+1)] + exog_features].values\n",
    "    y_dropoff = df_dropoff_train[[f'target_+{step}' for step in range(1, forecast_horizon+1)]].values\n",
    "    last_dropoff_to_predict = last_dropoff[[f'target_lag_{lag}' for lag in range(1, n_lags+1)] + exog_features].values.reshape(1, -1)\n",
    "\n",
    "    scaler_pickup = StandardScaler()\n",
    "    X_pickup_train_scaled = scaler_pickup.fit_transform(X_pickup)\n",
    "    last_pickup_to_predict_scaled = scaler_pickup.transform(last_pickup_to_predict)\n",
    "\n",
    "    scaler_dropoff = StandardScaler()\n",
    "    X_dropoff_train_scaled = scaler_dropoff.fit_transform(X_dropoff)\n",
    "    last_dropoff_to_predict_scaled = scaler_dropoff.transform(last_dropoff_to_predict)\n",
    "    #create and train models\n",
    "    p_final_model = create_nn_and_train(X_pickup_train_scaled, y_pickup)\n",
    "    d_final_model = create_nn_and_train(X_dropoff_train_scaled, y_dropoff)\n",
    "    #get predictions\n",
    "    p_pred = np.round(np.clip(p_final_model.predict(last_pickup_to_predict_scaled), 0, None))\n",
    "    d_pred = np.round(np.clip(d_final_model.predict(last_dropoff_to_predict_scaled), 0 ,None))\n",
    "    #find cumulative net\n",
    "    cum_net = np.cumsum(d_pred - p_pred)\n",
    "    required_bikes = max(0, -cum_net.min())\n",
    "    required_bikes_per_cluster[cluster_id] = required_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(required_bikes_per_cluster.items(), key=lambda x: x[0])\n",
    "sorted_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
